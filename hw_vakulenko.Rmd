---
title: "Экзаменационная работа"
date: '28 июня 2018'
author: 'Елена Вакуленко'
output: 
    html_document:
    keep_md: no
    number_sections: yes
    toc: yes
lang: ru-RU
editor_options:
  chunk_output_type: console
---

# 1. Выбераем набор данных

Для анализа я выбрала данные о коэффициентах миграционного прироста в регионах России в 2010 году. Эта переменная будет объясняющей в модели. В качестве регрессоров будут взяты: уровень безработицы; логарифм среднедушевых доходов, скорректированных на величину прожиточного минимума; ВРП; % добывающего сектора в валовой добавленной стоимости региона (ресурсонаделенность региона); дамми переменная "geography", разбивающая регионы на восточные и западные; дамми переменная "resources", показывающая наличие природных ресуров в регионе, дамми переменная "migration", показывает отрицательный или положительный миграционный прирост в регионе. 

Подключаем пакеты

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r}
library(tidyverse)
library(rio)
library(skimr)
library(lattice)
library(DataExplorer)
library(corrplot) # визуализация корреляций

library(cluster) # кластерный анализ
library(factoextra) # визуализации kmeans, pca,
library(dendextend) # визуализация дендрограмм

library(broom) # метла превращает результаты оценивания моделей в таблички

library(naniar) # визуализация пропущенных значений
library(visdat) # визуализация пропущенных значений

library(patchwork) # удобное расположение графиков рядом

library(GGally) # больше готовых графиков
library(sjPlot) # ещё больше графиков
library(lmtest) # диагностика линейных моделей
library(Ecdat) # много-много разных наборов данных
library(sjstats) # удобные мелкие функции для работы с моделями
library(sandwich) # оценка Var для гетероскедастичности
library(AER) # работа с инструментальными переменными

library(caret) # пакет для подбора параметров разных моделей
library(FFTrees) # быстрые деревья
library(margins) # для подсчёта предельных эффектов
library(rpart.plot) # для картинок деревьев
library(plotROC) # визуализация ROC-кривой
library(ggeffects) # графики для предельных эффектов
library(MLmetrics) # метрики качества
library(ranger) # строим деревья
library(factoextra) # графики для кластеризации и pca
library(elasticnet) # LASSO
library(latex2exp) # формулы в подписях к графику
library(distances) # расчет различных расстояний
```


#2. Загружаем данные формата Excel. Проводим описательный анализ данных.


```{r}
setwd("~/GitHub/r_course/data")
mig <- import('migration.xls')
glimpse(mig)
head(mig)
tail(mig)
skim(mig)
```

2.1.-2.2. Итого в наборе данных 11 переменных, 78 наблюдений. Нет пропущенных значений. Все переменные количественные, кроме geography, region, migration и resources, которые являются текстовыми. Сделаем их факторными. Region оставим текстовой, это название регионов.

```{r}
mig<- mutate(mig, geography = factor(geography), migration = factor(migration), resources = factor(resources))
#Другой способ
#mig <- mutate_if(mig, is.character, factor)
skim(mig)
```

2.3. Построим гистрограммы для количественных и качественных переменных и диаграммы рассеивания для количественных. 

```{r}
qplot(data = mig, x = u) +
  labs(x = 'Уровень безработицы', y = 'Количество', title = 'Распределение регионов по уровню безработицы')
qplot(data = mig, x = oil_gas_gdp)  +
  labs(x = 'Доля добывающего сектора в ВВП', y = 'Количество', title = 'Распределение регионов по ресурсодобыче')

qplot(data = mig, x = geography)  +
  labs(x = 'Географическое расположение региона', y = 'Количество', title = 'Распределение регионов по географическому положению')
```

Диаграммы рассеивания

```{r}
qplot(data = mig, x = u, y = net_mig_all) +
  labs(x = 'Уровень безработицы', y = 'Коэффициент миграционного прироста', title = 'Зависимость миграционного прироста от уровня безработицы')

qplot(data = mig, x = lnincome_pmini, y = net_mig_all) +
  labs(x = 'Логарифм среднедушевых доходов, скорректированных на величину прожиточного минимума в регионе', y = 'Коэффициент миграционного прироста', title = 'Зависимость миграционного прироста от доходов')

ggplot(data = mig) +
  geom_point(aes(x = oil_gas_gdp, y = net_mig_all, color = geography)) +
  geom_vline(aes(xintercept = mean(oil_gas_gdp))) +
  labs(x = 'Доля добывающего сектора в ВВП', y = 'Коэффициент миграционного прироста', title = 'Зависимость миграционного прироста от ресурсодобычи')
```



Отберем количественные переменные и рассчитаем корреляции

```{r}
mig2 <- select(mig, net_mig_all:oil_gas_gdp)
mig_cor <- cor(mig2)
mig_cor
corrplot(mig_cor) # график с аргументами по умолчанию
corrplot(mig_cor, method = 'number') # график со значениями корреляции
corrplot(mig_cor, method = 'color', type = 'upper') # только правая верхняя часть
```



#3. Проведение кластеризации данных методом k-средних

Отмасштабируем все числовые переменные с помощью функции `scale()`.
```{r}
mig3 <- mig2 %>%
  mutate_if(is.numeric, ~ as.vector(scale(.)))
```

3.1. Определим оптимальное число кластеров. Один из способов сделать это — воспользоваться командой `fviz_nbclust` из пакета `factoextra`.


```{r}
g1 <- fviz_nbclust(mig3, kmeans, method = 'wss') +
  labs(title = 'Зависимость WSS от числа кластеров',
       subtitle = 'Метод: на глаз!', x = 'Число кластеров',
       y = 'Внутригрупповая сумма квадратов расстояний')
g1
```

По методу "на глаз" стоит выбрать 2 кластера.

```{r}
g2 <- fviz_nbclust(mig3, kmeans, method = 'silhouette') +
  labs(subtitle = 'Метод силуэтов',
       title = 'Зависимость средней ширины силуэта от числа кластеров',
       y = 'Средняя ширина силуэта по всем точкам',
       x = 'Число кластеров')
g2
```

По данному методу наилучшее число кластеров также равно двум.


```{r}
g3 <- fviz_nbclust(mig2, kmeans, method = 'gap_stat') +
  labs(subtitle = 'Gap statistic method')
g3
```

По gap statistics оптимальное число кластеров равно 1, т.к. это первое число, после которого статистика резко падает.

В результате выберем 2 кластера.

```{r}
set.seed(15)
k_means_mig <- kmeans(mig2, centers = 2)
k_means_mig
```

Оказывается, что процедура кластеризации выделяет отдельно регион 18, т.е. Москву. Требуется проводить кластеризацию без Москвы.

Узнаем величину WSS с разбивкой по кластерам:
```{r}
k_means_mig$withinss
```

Межгрупповая сумма квадратов расстояний,  BSS (between):

```{r}
k_means_mig$betweenss
```

Общая сумма квадратов расстояний, TSS (total):

```{r}
k_means_mig$totss
```

# Иерархическая кластеризация

Другой способ разбить данные на группы — иерархическая кластеризация.
Но, в отличие от метода k-средних, она работает с матрицей расстояний,
поэтому первым делом посчитаем её!
Для этого будем использовать функцию `dist()`.
Передадим ей стандартизированные данные и укажем явно, как считать расстояния с помощью аргумента `method`.

```{r}
mig_dist <- dist(mig2, method = 'euclidian')
```

Расстояния тоже можно визуализировать!
Сделаем это командой `fviz_dist` из пакета `factoextra`.

```{r}
fviz_dist(mig_dist)
```

Полученную матрицу расстояний можно передадать функции `hclust()`, которая кластеризует данные.
Однако в пакете `factoextra` есть функция `hcut()`, которая работает с исходными данными.
Будем использовать её и попросим выделить четыре кластера в аргументе `k`.


```{r}
mig_hcl <- hcut(mig2, k = 4,
                    hc_metric = 'euclidean', hc_method = 'ward.D2')
```

С помощью функции `fviz_dend` визуализируем результат кластеризации.
Укажем несколько аргументов, чтобы сделать дендрограмму красивее.

```{r}
fviz_dend(mig_hcl,
          cex = 0.5, # размер подписи
          color_labels_by_k = TRUE) # цвет подписей по группам
```

Выявленные кластеры можно добавить к исходным данным!
```{r}
mig_plus2 <- mutate(mig2, cluster = mig_hcl$cluster)
glimpse(mig_plus2)
```

# Метод главных компонент

Построим главные компонены по нашему набору данных количесвенных переменных.

```{r}
mig_pca <- prcomp(mig2, scale. = TRUE)
mig_pca
```

Посмотрим, что лежит в этом списке.

```{r}
attributes(mig_pca)
```

Сами новые искусственные главные компоненты лежат в матрице `protein_pca$x`.
Например, первая главная компонента:
```{r}
mig_pca$x[, 1]
```

Выборочные стандартные отклонения компонент (корни из $\lambda_j$) лежат в векторе `mig_pca$sdev`:
```{r}
mig_pca$sdev
```

Матрица `mig_pca$rotation` содержит веса, с которыми исходные переменные входят в новые искуственные главные компоненты. Например, в первой главной компоненте лежат исходные переменные с весами:
```{r}
mig_pca$rotation[, 1]
```


3.2. Визуализируем данные в осях первых двух главных компонент.
Для этого воспользуемся функцией `fviz_pca_ind()` из пакета `factoextra`.
Рисовать будем `mig_pca`, а аргумент `repel = TRUE` укажем для того,
чтобы подписи на графике не перекрывали друг друга.

```{r}
fviz_pca_ind(mig_pca, repel = TRUE)

fviz_eig(mig_pca, addlabels = TRUE)
```

3.3. На первую главную компоненту приходится около 48%, на вторую - 23% объясненной дисперсии. Такие регионы, как 18 (Москва), 78 (Чукотка), 56 (Тюменская область), 76 (Сахалинская область) сильно отличаются от всех остальных.В основном все регионы концентрируются в области положильных значений обеих компонент.

В осях первых двух главных компонент исходные данные можно раскрасить согласно кластерам:

```{r}
fviz_cluster(object = k_means_mig,
             data = mig3,
             ellipse.type = 'convex')
```

Отдельная красная точка соответствует Москве. Попробуем увеличить количество кластеров до 3-х и изобразить результат.

```{r}
set.seed(15)
k_means_mig2 <- kmeans(mig2, centers = 3)
k_means_mig2
fviz_cluster(object = k_means_mig2,
             data = mig3,
             ellipse.type = 'convex')
```

На графиках видно, что такое разбиение более адекватное. Как и прежде отделяется Москва (точка 18), а все остальные регионы разбиваются на два кластера.

# 4. Оценим модель линейной регрессии

Начнем с короткой модели. Зависимая переменная - коэффициент миграционного прироста. Объясняюшие переменные: уровень безработицы, среднедушевые доходы, % добывающего сектора в ВРП.

```{r}
model_r <- lm(data = mig, net_mig_all ~ u + lnincome_pmini + oil_gas_gdp)
```

С помощью команды `summary()` посмотрим на описание модели.

```{r}
summary(model_r)
glance(model_r)
```

Модель в целом значима. Все коэффициенты, кроме константы, значимы на 5% уровне значимости. Доходы и ресурсы в ВВП значимы на 0,1% уровне значимости. Причем мигранты предпочитают регионы с большими доходами, с меньшим уровнем безработицы и регионы с меньшей долей сектора добычи полезных ископаемых в ВВП.

Оценим длинную модель линейной регрессии, добавив плотность населения и дамми переменную на географическое положение.

```{r}
model_ur <- lm(data = mig, net_mig_all ~ u + lnincome_pmini + oil_gas_gdp +
pop_density + geography)
summary(model_ur)
glance(model_ur)
```
Дополнительные переменные оказались незначимыми на любом разумном уровне значимости.

Проведем диагностику модели.

```{r}
ggnostic(model = model_ur)

```

В модели явно есть выбросы, что мы уже отмечали ранее.

Расмотрим коэффициенты модели и их доверительные интервалы:

```{r}
coefci(model_ur)
plot_model(model_ur, ci.lvl = 0.95)
```

Сравним длинную и короткую модели.Если есть две вложенных модели, то есть одна является частным случаем другой,
то можно провести тест Вальда, чтобы выбрать одну из них.

$H_0$: верна ограниченная (короткая) модель;

$H_a$: верная неограниченная (длинная) модель;

```{r}
waldtest(model_r, model_ur)
```

p-value теста Вальда больше любого разумного уровня значимости, нулевая гипотеза не отвергается. Отдаем предпочтение короткой модели. Дополнительно включенные факторы оказываются совместно незначимыми.

Оценим регрессии в случае гетероскедастичных ошибок.

```{r}
coeftest(model_ur, vcov. = vcovHC)
coeftest(model_r, vcov. = vcovHC)
```
Коэффициент при уровне безработицы стал незначимым даже на 10% уровне значимости.

Получим доверительные интервалы:

```{r}
coefci(model_ur, vcov. = vcovHC)
coefci(model_r, vcov. = vcovHC)
```

Дополнительные регрессоры также незначимы. 

Сравним две вложенных модели с учётом поправки на гетероскедастичность:
```{r}
waldtest(model_r, model_ur, vcov = vcovHC)
```

Результаты теста качественно неизменились. Отдаем предпочтение короткой модели.

#5. Проведем задачу классификации. 

Для этого в качестве зависимой переменной возьмем переменную "migration", которая принимает, как отрицельные, так и положительные значени в зависимости от коэффициента миграционного прироста, т.е. люди либо в основном приезжают или уезжают из конкретного региона. Обозначим эту переменную, как факторную. Оценим для нее логистическую регрессию по тестовой выборке.

```{r}

mig <- mutate(mig, migration = factor(migration))

skim(mig)
```

Создадим вектор `mig_rows` с номерами строк для обучающей части. Возьмем 70% от общей выборки.

```{r}
set.seed(555)
mig_rows <- createDataPartition(mig$migration, p = 0.7, list = FALSE)
```


И разделим выборку согласно вектору `mig_rows`:


```{r}
mig_train <- mig[mig_rows, ]
mig_test <- mig[-mig_rows, ]
```


# Логистическая регрессия

С помощью метода максимального правдоподобия оценивается модель

\[
y_i =
\begin{cases}
1, \text{ если } y^*_i \geq 0; \\
0, \text{ иначе.}
\end{cases}
\]

\[
y_i^* = \beta_1 + \beta_2 x_i + \beta_3 z_i + u_i,
\]
где $u_i$ имеет специальное логистическое распределение.

Также модель можно записать в виде:

\[
\frac{\ln P(y_i = 1)}{\ln P(y_i = 0)} = \beta_1 + \beta_2 x_i + \beta_3 z_i
\]


Реализуем логистическую регрессию с помощью функции `glm`.
Передадим ей набор данных `mig_train`, формулу и укажем специальный аргумент `family = binomial(link = 'logit')`.
Сохраним результаты оценивания в переменную `mig_lmodel` и посмотрим на них.
Зависимая переменная - положительный или отрицательный миграционный прирост. Объясняющие переменные те, которые были в длинной линейной регрессионной модели.

```{r}

mig_lmodel_glm <- glm(data = mig_train, migration ~ u + lnincome_pmini + oil_gas_gdp + geography + pop_density, family = binomial(link = 'logit'))
summary(mig_lmodel_glm)
```

Значимыми оказались только переменные доходов и плотности населения, причем на 5% уровне значимости.

Чтобы найти предельные эффекты, воспользуемся функцией `margins` из одноимённого пакета.
Снова выведем результаты командой `summary()`.

```{r}
mig_margins <- margins(mig_lmodel_glm)
summary(mig_margins)
```

Средний предельный доход для логарифмов скорректированных доходов равен 0.91. Следовательно, увеличение среднедушевых доходов, скорректированных на величину прожиточного минимума, на 1% увеличивает вероятность быть привлекательным региономна 0.91 (в шкале [0,1])  или на 91 п.п. при прочих равных. Для дамми переменных проитерпретируем отношение шансов. Для переменной "географическое положение" exp(-1.75)=0.17, т.е. отношение шансов быть привлекательным регионом для западной части страны по сравнению с восточной равно 0.17 при прочих равных.

```{r, eval=FALSE}
pred_mig_vis <- ggpredict(mig_lmodel_glm, terms = c('lnincome_pmini', 'geography'), pretty = FALSE)

plot(pred_mig_vis)
```


Оценивать логистическую модель также можно с помощью пакета `caret`.
Изменения минимальные:


```{r}
mig_lmodel <- train(data = mig_train, migration ~ u + lnincome_pmini + oil_gas_gdp + geography + pop_density, family = binomial(link = 'logit'), method = 'glm')
summary(mig_lmodel)
```

Построим прогнозы модели для тестовых данных `mig_test`.
Для этого будем использовать функцию `predict`, которой передадим оценённую модель `mig_lmodel`.
В переменной `mig_pred` уже будут лежать предсказанные классы.
Чтобы получить их вероятности, нужно добавить аргумент `type = 'prob'`.

```{r}
mig_pred <- predict(mig_lmodel, newdata = mig_test)
head(mig_pred)

mig_prob <- predict(mig_lmodel, newdata = mig_test, type = 'prob')
head(mig_prob)
```

Теперь мы можем посмотреть на матрицу ошибок и узнать, насколько хорошую модель мы оценили.
Для этого будем использовать функцию `confusionMatrix` из пакета `caret`.
В качестве аргумента `data` нужно указать предсказанные значения, а в `reference` — правильные ответы.

```{r}
confusionMatrix(data = mig_pred, reference = mig_test$migration)
```

Процент правильно предсказанных значений равен 69.5%. Чувствительность - 82.3%, специфичность - 33.3%.

Создадим столбцы с вероятностями классов под названиями самих этих классов, истинные ответы под названием `obs`, и бинарные предсказания, названные как `pred`.
Поэтому создадим отдельную таблицу `mig_test_set` со всеми результатами оценивания.


```{r}
mig_test_set <- data.frame(positive = mig_prob$positive,
                        negative = mig_prob$negative,
                        pred = mig_pred,
                        obs = mig_test$migration)
glimpse(mig_test_set)

twoClassSummary(mig_test_set, lev = levels(mig_test_set$obs)) # don't work with tibble
prSummary(mig_test_set, lev = levels(mig_test_set$obs)) # нужен пакет MLmetrics
```

Площадь под кривой ROC равно 0.76.

Построим ROC-кривую.
Для этого вдобавок к базовому cлою `ggplot` мы будем использовать слой `geom_roc` из пакета `plotROC`.
В эстетиках нужно указать аргументы `d` — истинные значения — и `m` — метки класса 1.
Если добавить аргумент `color`, то можно получить разные ROC-кривые по географическому положению регионов, например.


```{r}
ggplot(mig_test_set, aes(d = obs, m = positive)) +
  geom_roc(n.cuts = 10) +
  labs(title = 'Кривая ROC',
       x = 'False positive ratio = FP / (FP + TN)',
       y = 'True positive ratio = TP / (TP + FN)')

mig_test_set <- mutate(mig_test_set, geo = mig_test$geography)

ggplot(mig_test_set, aes(d = obs, m = positive, color = geo)) +
  geom_roc(n.cuts = 0) +
  labs(title = 'Кривые ROC в зависимости от ресурсов',
       x = 'False positive ratio = FP / (FP + TN)',
       y = 'True positive ratio = TP / (TP + FN)',
       color = 'Географическое положение')
```

Ура! :)